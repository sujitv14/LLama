# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZr-mvAYVwrq1eNlw7cJTnUE5KL0mXWJ
"""

!pip install langchain transformers sentencepiece ctransformers

from google.colab import drive
drive.mount('/content/drive')

!pip install streamlit

import streamlit as st
from langchain import PromptTemplate, LLMChain
from ctransformers import AutoModelForCausalLM

# Load the LLaMA model using ctransformers
model_path = '/content/drive/MyDrive/llama-2-7b-chat.ggmlv3.q8_0.bin'
llm = AutoModelForCausalLM.from_pretrained(model_path, model_type="llama")

# Create a Langchain PromptTemplate
template = """
Question: {question}
Answer:
"""

prompt_template = PromptTemplate(
    input_variables=["question"],
    template=template,
)

# Streamlit interface
st.title("LLaMA-2 Chatbot with Langchain")

# User input
question = st.text_input("Enter your question:")

# Generate response when user submits a question
if question:
    st.write("Generating response...")

    # Use Langchain PromptTemplate to format the question
    prompt = prompt_template.format(question=question)

    # Generate the response from the model using ctransformers
    response = llm(prompt)

    # Display the response
    st.write(f"Response: {response}")